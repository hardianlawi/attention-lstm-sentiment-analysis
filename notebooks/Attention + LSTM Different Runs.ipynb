{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:52:06.677756Z",
     "start_time": "2020-02-09T05:52:06.533968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hardianlawi/attention-lstm-sentiment-analysis\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%cd ..\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%cd ..\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:52:36.437161Z",
     "start_time": "2020-02-09T05:52:06.680035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import json\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.backend as K\\nfrom src.preprocess import Preprocessor\\nfrom src.data import get_data\\nfrom src.models import get_model\\n\\ntf.random.set_seed(2020)\\n\\nvocab_size = 5000\\nemb_size = 32\\nmax_words = 500\\noov_token = \\\"<OOV>\\\"\\n\\n(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\\n\\npreprocessor = Preprocessor(\\n    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\\n)\\npreprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\\n\\nX_train = preprocessor.transform(str_X_train)\\nX_val = preprocessor.transform(str_X_val)\\nX_test = preprocessor.transform(str_X_test)\";\n",
       "                var nbb_formatted_code = \"import json\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.backend as K\\nfrom src.preprocess import Preprocessor\\nfrom src.data import get_data\\nfrom src.models import get_model\\n\\ntf.random.set_seed(2020)\\n\\nvocab_size = 5000\\nemb_size = 32\\nmax_words = 500\\noov_token = \\\"<OOV>\\\"\\n\\n(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\\n\\npreprocessor = Preprocessor(\\n    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\\n)\\npreprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\\n\\nX_train = preprocessor.transform(str_X_train)\\nX_val = preprocessor.transform(str_X_val)\\nX_test = preprocessor.transform(str_X_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from src.preprocess import Preprocessor\n",
    "from src.data import get_data\n",
    "from src.models import get_model\n",
    "\n",
    "tf.random.set_seed(2020)\n",
    "\n",
    "vocab_size = 5000\n",
    "emb_size = 32\n",
    "max_words = 500\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\n",
    ")\n",
    "preprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\n",
    "\n",
    "X_train = preprocessor.transform(str_X_train)\n",
    "X_val = preprocessor.transform(str_X_val)\n",
    "X_test = preprocessor.transform(str_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T06:27:45.457902Z",
     "start_time": "2020-02-09T05:52:36.439540Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 89s 4ms/sample - loss: 0.5239 - accuracy: 0.7496 - val_loss: 0.3253 - val_accuracy: 0.8664\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 85s 4ms/sample - loss: 0.2787 - accuracy: 0.8902 - val_loss: 0.2701 - val_accuracy: 0.8848\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 82s 3ms/sample - loss: 0.2254 - accuracy: 0.9149 - val_loss: 0.2605 - val_accuracy: 0.8904\n",
      "Test accuracy: 0.87756\n",
      "(64, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 124s 5ms/sample - loss: 0.4966 - accuracy: 0.7586 - val_loss: 0.3056 - val_accuracy: 0.8712\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 120s 5ms/sample - loss: 0.2632 - accuracy: 0.8973 - val_loss: 0.2794 - val_accuracy: 0.8944\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 117s 5ms/sample - loss: 0.2192 - accuracy: 0.9163 - val_loss: 0.2699 - val_accuracy: 0.8928\n",
      "Test accuracy: 0.87828\n",
      "(32, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 88s 4ms/sample - loss: 0.5096 - accuracy: 0.7512 - val_loss: 0.3256 - val_accuracy: 0.8704\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 78s 3ms/sample - loss: 0.2690 - accuracy: 0.8952 - val_loss: 0.2646 - val_accuracy: 0.8952\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 73s 3ms/sample - loss: 0.2190 - accuracy: 0.9173 - val_loss: 0.2647 - val_accuracy: 0.8976\n",
      "Test accuracy: 0.87908\n",
      "(64, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 115s 5ms/sample - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.3319 - val_accuracy: 0.8688\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 109s 5ms/sample - loss: 0.2678 - accuracy: 0.8938 - val_loss: 0.2643 - val_accuracy: 0.8960\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 109s 5ms/sample - loss: 0.2189 - accuracy: 0.9172 - val_loss: 0.2751 - val_accuracy: 0.8896\n",
      "Test accuracy: 0.87724\n",
      "(32, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 79s 3ms/sample - loss: 0.5219 - accuracy: 0.7447 - val_loss: 0.3244 - val_accuracy: 0.8624\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 84s 4ms/sample - loss: 0.2718 - accuracy: 0.8947 - val_loss: 0.2656 - val_accuracy: 0.8984\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 82s 3ms/sample - loss: 0.2178 - accuracy: 0.9172 - val_loss: 0.2555 - val_accuracy: 0.9040\n",
      "Test accuracy: 0.8798\n",
      "(64, 16)\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 112s 5ms/sample - loss: 0.5312 - accuracy: 0.7455 - val_loss: 0.3744 - val_accuracy: 0.8376\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 106s 4ms/sample - loss: 0.2734 - accuracy: 0.8915 - val_loss: 0.2622 - val_accuracy: 0.8928\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 107s 5ms/sample - loss: 0.2198 - accuracy: 0.9165 - val_loss: 0.2710 - val_accuracy: 0.8944\n",
      "Test accuracy: 0.87924\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def generate_scores(i):\\n\\n    models = []\\n    num_params = []\\n    accs = []\\n\\n    def train(hs, ahs):\\n\\n        model = get_model(\\n            \\\"attention\\\",\\n            max_words,\\n            vocab_size,\\n            emb_size,\\n            hidden_size=hs,\\n            attention_hs=ahs,\\n        )\\n        model.compile(\\n            loss=\\\"binary_crossentropy\\\", optimizer=\\\"adam\\\", metrics=[\\\"accuracy\\\"]\\n        )\\n\\n        batch_size = 128\\n        num_epochs = 3\\n\\n        model.fit(\\n            X_train,\\n            y_train,\\n            validation_data=(X_val, y_val),\\n            batch_size=batch_size,\\n            epochs=num_epochs,\\n        )\\n\\n        scores = model.evaluate(X_test, y_test, verbose=0)\\n        print(\\\"Test accuracy:\\\", scores[1])\\n\\n        return scores[1], model.count_params(), model\\n\\n    hparams = [(32, 16), (64, 16)]\\n    for (hs, ahs) in hparams:\\n\\n        print(f\\\"{(hs, ahs)}\\\")\\n\\n        acc, num_param, model = train(hs, ahs)\\n        accs.append(float(acc))\\n        num_params.append(num_param)\\n        models.append(model)\\n\\n    perfs = {\\n        \\\"hparams\\\": hparams,\\n        \\\"num_params\\\": [int(x) for x in num_params],\\n        \\\"accs\\\": [float(x) for x in accs],\\n    }\\n\\n    with open(f\\\"attention_perfs_{i}.json\\\", \\\"w\\\") as f:\\n        json.dump(perfs, f)\\n\\n\\nfor i in range(3):\\n    generate_scores(i)\";\n",
       "                var nbb_formatted_code = \"def generate_scores(i):\\n\\n    models = []\\n    num_params = []\\n    accs = []\\n\\n    def train(hs, ahs):\\n\\n        model = get_model(\\n            \\\"attention\\\",\\n            max_words,\\n            vocab_size,\\n            emb_size,\\n            hidden_size=hs,\\n            attention_hs=ahs,\\n        )\\n        model.compile(\\n            loss=\\\"binary_crossentropy\\\", optimizer=\\\"adam\\\", metrics=[\\\"accuracy\\\"]\\n        )\\n\\n        batch_size = 128\\n        num_epochs = 3\\n\\n        model.fit(\\n            X_train,\\n            y_train,\\n            validation_data=(X_val, y_val),\\n            batch_size=batch_size,\\n            epochs=num_epochs,\\n        )\\n\\n        scores = model.evaluate(X_test, y_test, verbose=0)\\n        print(\\\"Test accuracy:\\\", scores[1])\\n\\n        return scores[1], model.count_params(), model\\n\\n    hparams = [(32, 16), (64, 16)]\\n    for (hs, ahs) in hparams:\\n\\n        print(f\\\"{(hs, ahs)}\\\")\\n\\n        acc, num_param, model = train(hs, ahs)\\n        accs.append(float(acc))\\n        num_params.append(num_param)\\n        models.append(model)\\n\\n    perfs = {\\n        \\\"hparams\\\": hparams,\\n        \\\"num_params\\\": [int(x) for x in num_params],\\n        \\\"accs\\\": [float(x) for x in accs],\\n    }\\n\\n    with open(f\\\"attention_perfs_{i}.json\\\", \\\"w\\\") as f:\\n        json.dump(perfs, f)\\n\\n\\nfor i in range(3):\\n    generate_scores(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_scores(i):\n",
    "\n",
    "    models = []\n",
    "    num_params = []\n",
    "    accs = []\n",
    "\n",
    "    def train(hs, ahs):\n",
    "\n",
    "        model = get_model(\n",
    "            \"attention\",\n",
    "            max_words,\n",
    "            vocab_size,\n",
    "            emb_size,\n",
    "            hidden_size=hs,\n",
    "            attention_hs=ahs,\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        batch_size = 128\n",
    "        num_epochs = 3\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "        return scores[1], model.count_params(), model\n",
    "\n",
    "    hparams = [(32, 16), (64, 16)]\n",
    "    for (hs, ahs) in hparams:\n",
    "\n",
    "        print(f\"{(hs, ahs)}\")\n",
    "\n",
    "        acc, num_param, model = train(hs, ahs)\n",
    "        accs.append(float(acc))\n",
    "        num_params.append(num_param)\n",
    "        models.append(model)\n",
    "\n",
    "    perfs = {\n",
    "        \"hparams\": hparams,\n",
    "        \"num_params\": [int(x) for x in num_params],\n",
    "        \"accs\": [float(x) for x in accs],\n",
    "    }\n",
    "\n",
    "    with open(f\"attention_perfs_{i}.json\", \"w\") as f:\n",
    "        json.dump(perfs, f)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    generate_scores(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-lstm-sentiment-analysis",
   "language": "python",
   "name": "attention-lstm-sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
