{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:29:38.655882Z",
     "start_time": "2020-02-09T05:29:38.552982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hardianlawi/attention-lstm-sentiment-analysis\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%cd ..\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%cd ..\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T05:30:08.114038Z",
     "start_time": "2020-02-09T05:29:38.657664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import json\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.backend as K\\nfrom src.preprocess import Preprocessor\\nfrom src.data import get_data\\nfrom src.models import get_model\\n\\ntf.random.set_seed(2020)\\n\\nvocab_size = 5000\\nemb_size = 32\\nmax_words = 500\\noov_token = \\\"<OOV>\\\"\\n\\n(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\\n\\npreprocessor = Preprocessor(\\n    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\\n)\\npreprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\\n\\nX_train = preprocessor.transform(str_X_train)\\nX_val = preprocessor.transform(str_X_val)\\nX_test = preprocessor.transform(str_X_test)\";\n",
       "                var nbb_formatted_code = \"import json\\nimport numpy as np\\nimport tensorflow as tf\\nimport tensorflow.keras.backend as K\\nfrom src.preprocess import Preprocessor\\nfrom src.data import get_data\\nfrom src.models import get_model\\n\\ntf.random.set_seed(2020)\\n\\nvocab_size = 5000\\nemb_size = 32\\nmax_words = 500\\noov_token = \\\"<OOV>\\\"\\n\\n(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\\n\\npreprocessor = Preprocessor(\\n    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\\n)\\npreprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\\n\\nX_train = preprocessor.transform(str_X_train)\\nX_val = preprocessor.transform(str_X_val)\\nX_test = preprocessor.transform(str_X_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from src.preprocess import Preprocessor\n",
    "from src.data import get_data\n",
    "from src.models import get_model\n",
    "\n",
    "tf.random.set_seed(2020)\n",
    "\n",
    "vocab_size = 5000\n",
    "emb_size = 32\n",
    "max_words = 500\n",
    "oov_token = \"<OOV>\"\n",
    "\n",
    "(str_X_train, y_train), (str_X_val, y_val), (str_X_test, y_test) = get_data()\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    maxlen=max_words, vocab_size=vocab_size, oov_token=oov_token\n",
    ")\n",
    "preprocessor.fit_on_texts(str_X_train + str_X_val + str_X_test)\n",
    "\n",
    "X_train = preprocessor.transform(str_X_train)\n",
    "X_val = preprocessor.transform(str_X_val)\n",
    "X_test = preprocessor.transform(str_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-09T06:04:39.734736Z",
     "start_time": "2020-02-09T05:30:08.116701Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 89s 4ms/sample - loss: 0.4668 - accuracy: 0.7777 - val_loss: 0.3288 - val_accuracy: 0.8592\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 83s 4ms/sample - loss: 0.2704 - accuracy: 0.8947 - val_loss: 0.2783 - val_accuracy: 0.8824\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 83s 3ms/sample - loss: 0.2400 - accuracy: 0.9080 - val_loss: 0.3259 - val_accuracy: 0.8808\n",
      "Test accuracy: 0.86988\n",
      "64\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 109s 5ms/sample - loss: 0.4514 - accuracy: 0.7875 - val_loss: 0.2945 - val_accuracy: 0.8776\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 104s 4ms/sample - loss: 0.2704 - accuracy: 0.8917 - val_loss: 0.2748 - val_accuracy: 0.8856\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 106s 4ms/sample - loss: 0.2275 - accuracy: 0.9141 - val_loss: 0.2716 - val_accuracy: 0.8944\n",
      "Test accuracy: 0.87556\n",
      "32\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 77s 3ms/sample - loss: 0.4581 - accuracy: 0.7814 - val_loss: 0.3026 - val_accuracy: 0.8680\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 73s 3ms/sample - loss: 0.2641 - accuracy: 0.8973 - val_loss: 0.2740 - val_accuracy: 0.8960\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 76s 3ms/sample - loss: 0.2232 - accuracy: 0.9158 - val_loss: 0.3682 - val_accuracy: 0.8688\n",
      "Test accuracy: 0.86324\n",
      "64\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 109s 5ms/sample - loss: 0.5322 - accuracy: 0.7397 - val_loss: 0.3366 - val_accuracy: 0.8560\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 104s 4ms/sample - loss: 0.2924 - accuracy: 0.8833 - val_loss: 0.2862 - val_accuracy: 0.8880\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 109s 5ms/sample - loss: 0.2322 - accuracy: 0.9123 - val_loss: 0.3154 - val_accuracy: 0.8728\n",
      "Test accuracy: 0.86932\n",
      "32\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 86s 4ms/sample - loss: 0.5101 - accuracy: 0.7504 - val_loss: 0.3313 - val_accuracy: 0.8600\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 83s 3ms/sample - loss: 0.2897 - accuracy: 0.8868 - val_loss: 0.2969 - val_accuracy: 0.8720\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 84s 4ms/sample - loss: 0.2297 - accuracy: 0.9133 - val_loss: 0.2742 - val_accuracy: 0.8888\n",
      "Test accuracy: 0.87064\n",
      "64\n",
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/3\n",
      "23750/23750 [==============================] - 120s 5ms/sample - loss: 0.4376 - accuracy: 0.7971 - val_loss: 0.2822 - val_accuracy: 0.8816\n",
      "Epoch 2/3\n",
      "23750/23750 [==============================] - 119s 5ms/sample - loss: 0.2749 - accuracy: 0.8922 - val_loss: 0.2773 - val_accuracy: 0.8824\n",
      "Epoch 3/3\n",
      "23750/23750 [==============================] - 117s 5ms/sample - loss: 0.2374 - accuracy: 0.9105 - val_loss: 0.3012 - val_accuracy: 0.8744\n",
      "Test accuracy: 0.85724\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"def generate_scores(i):\\n    \\n    models = []\\n    num_params = []\\n    accs = []\\n\\n\\n    def train(hs):\\n\\n        model = get_model(\\n            'lstm',\\n            max_words,\\n            vocab_size,\\n            emb_size,\\n            hidden_size=hs,\\n        )\\n        model.compile(loss=\\\"binary_crossentropy\\\", optimizer=\\\"adam\\\", metrics=[\\\"accuracy\\\"])\\n\\n        batch_size = 128\\n        num_epochs = 3\\n\\n        model.fit(\\n            X_train,\\n            y_train,\\n            validation_data=(X_val, y_val),\\n            batch_size=batch_size,\\n            epochs=num_epochs,\\n        )\\n\\n        scores = model.evaluate(X_test, y_test, verbose=0)\\n        print(\\\"Test accuracy:\\\", scores[1])\\n\\n        return scores[1], model.count_params(), model\\n\\n\\n    hparams = [32, 64]\\n    for (hs) in hparams:\\n\\n        print(f\\\"{(hs)}\\\")\\n\\n        acc, num_param, model = train(hs)\\n        accs.append(float(acc))\\n        num_params.append(num_param)\\n        models.append(model)\\n\\n\\n    perfs = {\\n        \\\"hparams\\\": hparams,\\n        \\\"num_params\\\": [int(x) for x in num_params],\\n        \\\"accs\\\": [float(x) for x in accs],\\n    }\\n\\n    with open(f\\\"lstm_perfs_{i}.json\\\", \\\"w\\\") as f:\\n        json.dump(perfs, f)\\n        \\nfor i in range(3):\\n    generate_scores(i)\";\n",
       "                var nbb_formatted_code = \"def generate_scores(i):\\n\\n    models = []\\n    num_params = []\\n    accs = []\\n\\n    def train(hs):\\n\\n        model = get_model(\\\"lstm\\\", max_words, vocab_size, emb_size, hidden_size=hs,)\\n        model.compile(\\n            loss=\\\"binary_crossentropy\\\", optimizer=\\\"adam\\\", metrics=[\\\"accuracy\\\"]\\n        )\\n\\n        batch_size = 128\\n        num_epochs = 3\\n\\n        model.fit(\\n            X_train,\\n            y_train,\\n            validation_data=(X_val, y_val),\\n            batch_size=batch_size,\\n            epochs=num_epochs,\\n        )\\n\\n        scores = model.evaluate(X_test, y_test, verbose=0)\\n        print(\\\"Test accuracy:\\\", scores[1])\\n\\n        return scores[1], model.count_params(), model\\n\\n    hparams = [32, 64]\\n    for hs in hparams:\\n\\n        print(f\\\"{(hs)}\\\")\\n\\n        acc, num_param, model = train(hs)\\n        accs.append(float(acc))\\n        num_params.append(num_param)\\n        models.append(model)\\n\\n    perfs = {\\n        \\\"hparams\\\": hparams,\\n        \\\"num_params\\\": [int(x) for x in num_params],\\n        \\\"accs\\\": [float(x) for x in accs],\\n    }\\n\\n    with open(f\\\"lstm_perfs_{i}.json\\\", \\\"w\\\") as f:\\n        json.dump(perfs, f)\\n\\n\\nfor i in range(3):\\n    generate_scores(i)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_scores(i):\n",
    "\n",
    "    models = []\n",
    "    num_params = []\n",
    "    accs = []\n",
    "\n",
    "    def train(hs):\n",
    "\n",
    "        model = get_model(\"lstm\", max_words, vocab_size, emb_size, hidden_size=hs,)\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        batch_size = 128\n",
    "        num_epochs = 3\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"Test accuracy:\", scores[1])\n",
    "\n",
    "        return scores[1], model.count_params(), model\n",
    "\n",
    "    hparams = [32, 64]\n",
    "    for hs in hparams:\n",
    "\n",
    "        print(f\"{(hs)}\")\n",
    "\n",
    "        acc, num_param, model = train(hs)\n",
    "        accs.append(float(acc))\n",
    "        num_params.append(num_param)\n",
    "        models.append(model)\n",
    "\n",
    "    perfs = {\n",
    "        \"hparams\": hparams,\n",
    "        \"num_params\": [int(x) for x in num_params],\n",
    "        \"accs\": [float(x) for x in accs],\n",
    "    }\n",
    "\n",
    "    with open(f\"lstm_perfs_{i}.json\", \"w\") as f:\n",
    "        json.dump(perfs, f)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    generate_scores(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-lstm-sentiment-analysis",
   "language": "python",
   "name": "attention-lstm-sentiment-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
